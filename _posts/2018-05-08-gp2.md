---
layout: post
title: Introduction to Gaussian Processes, 2
---

In this post I want to take a deeper look at what a Gaussian Process is doing under hood.

At the end of the [last post](/2018/05/08/gp1.html) we had gotten to the idea that, rather than treating the parameters of a model as probability distributions, we could write down a probability distribution over models, and sample directly from that. Concretely, we left things with the observation that, given a basis function, $\phi(\mathbf{x})$, we could write down the probability distribution over functions in this basis[^1] as,

$$
f \sim \mathcal{N}(0,\, \phi(\mathbf{x})\phi(\mathbf{x})^T)
$$

Sampling from this multivariate Gaussian will give us functions drawn from the span of the basis $\phi$.


To clarify what I mean by this let's look at the case of a 2D Gaussian,

![gauss](/images/gp/2d_gaussian.png)

This Gaussian has zero mean[^2], and some covariance which we'll call $\mathbf{K}$, $\mathcal{N}(\mathbf{0},\, \mathbf{K})$. For a 2D Gaussian $\mathbf{K}$ is a $2\times2$ matrix, and it's clear that this $\mathbf{K}$ has some non-trivial structure. There is some covariance, which is to say there are non-zero elements off the main diagonal. We can take draws from this distribution, pairs of values, let's call them $(x_1, \, x_2)$. One thing we can look at is how setting $x_1$ to a given value affects $x_2$.

![tiled](/images/gp/tiled.png)

Looking at the diagram we see that setting $x_1$ to some value constrains $x_2$ to be drawn from (surprise surprise) a Gaussian distribution, with different Gaussians for different $x_1$'s.[^3] So by setting $x_1$ we can get an estimate for $x_2$, along with a error bar. Let's do that and plot up the results,

![draws](/images/gp/draws.png)

Viewed this way our samples are really samples from a distribution that generates linear functions. The important thing to bear in mind here is that the kind of linear functions we get out (their slopes, intercepts, how varied they are) is entirely determined by the structure of the covariance matrix $\mathbf{K}$. We saw in the last post how we can build covariance matrices from basis functions, but there's nothing that says we have to do things that way, we can build $\mathbf{K}$ by any method we like,[^4] and this will determine the kinds of functions we draw from the distribution. For my money this is a much clearer, cleverer way of specifying a model than the more traditional bases and weights approach, and as we're about to see can be incredibly flexible.


 Let's look at a bigger example, let's go from a 2D Gaussian to a 100D Gaussian.[^5] Now we need to specify a $100 \times 100$ matrix for $\mathbf{K}$. To do this we use a kernel function (the covariance matrix is often called the kernel), and one of the most common kernel functions is the Squared Exponential Kernel

 $$
 k(x, x') = \sigma^2 \exp \left(-\frac{(x - x')^2}{2l^2}\right)
 $$

Here $x$ and $x'$ are the values between which we want to determine the covariance, $\sigma$ is a scaling factor, and $l$ is the length scale of our covariance. Setting $\sigma$ and $l$, we can use this kernel to compute the covariance between pairs of points and populate our kernel matrix. It will end up having a structure like this

<center>
<img src="/images/gp/kernel.png">
</center>

Here we see every point has covariance $1$ with itself (trivially), decreasing covariance with points nearby, and no covariance with distant points. Let's take draws from a Gaussian with this covariance structure

```python
def gaussian(C):
        '''
        return draw from a 0 mean gaussian with given covariance
        '''
        return np.random.multivariate_normal(np.zeros(len(C)), C)


def sq_exp_kernel(x, x_d, theta=[1, 1]):
        '''
        exponentiated quadratic kernel
        '''
        sig, l = theta
        x = x.reshape(-1,1)
        x_d = x_d.reshape(-1,1)

        sqdist = (np.sum(x**2, 1).reshape(-1, 1) +
                  np.sum(x_d**2, 1) -
                  2 * np.dot(x, x_d.T))

        return sig**2 * np.exp(-0.5 * (1 / l**2) * sqdist)


x = np.linspace(-1, 1, 100)
k = sq_exp_kernel(x, x)

for i in range(10):
    plt.plot(x, gaussian(k))
```

![sq](/images/gp/sq_draws.png)

Cool! We get some nice, smoothly varying functions. This is all well and good but what we're missing here is the capacity to condition our distribution on data; once we've observed a data point in a given location that should constrain the types of functions that we consider valid in that region.

Let's say we have made some observations $x$, and we want to make predictions at some points $x_\ast$. A GP must be *consistent*, this is an important part of the formal definition of a GP, a definition I've studiously avoided giving so far. By consistency we mean that the model makes predictions for $x$ that are unaffected by future values of $x_\ast$ that are currently unobserved, i.e. the test points. By this property we can write down a joint probability density over the training observations and the test points, $x$ and $x_\ast$.[^6] This joint probability density will be

$$
\begin{bmatrix}
f(x) \\
f(x_\ast)
\end{bmatrix}

=
\mathcal{N} \left(0,
\begin{bmatrix}
K(x, x) & K(x, x_\ast) \\
K(x_\ast, x) & K(x_\ast, x_\ast)
\end{bmatrix}
\right)
$$

This kernel matrix is how information is passed between the training data and the test points. From this we can get

$$
f(x_\ast) \sim  \mathcal{N}(K(x_\ast,\, x)K(x, x)^{-1}f(x), \\
                          K(x_\ast, x_\ast) - K(x_\ast, x)K(x, x)^{-1}K(x, x_\ast))
$$

That's a bit of a mouthful, but by sampling from this distribution we can get functions which are conditioned on our observation[^7]

```python
x = np.random.random((1,1))
y = np.random.random((1,1))

x_s = np.linspace(-2, 2, 100)

kss = sq_exp_kernel(x_s, x_s)
ksx = sq_exp_kernel(x_s, x)
kxs = sq_exp_kernel(x, x_s)
kxx = sq_exp_kernel(x, x)
inv = np.linalg.inv

mu = ksx.dot(inv(kxx)).dot(y)
C = kss - ksx.dot(inv(kxx)).dot(kxs)

for i in range(10):
    plt.plot(x_s, np.random.multivariate_normal(mu, C))

plt.plot(x.ravel(), y.ravel(), 'ok')
```
![draws](/images/gp/conditioned_draws.png)

I have to admit I really like these plots.

So the last thing to cover is what to do with the two free parameters in the kernel function. These two control the amplitude of the functions, $\sigma$, and their length-scale ('wiggliness'), $l$. Thankfully this isn't too tricky, we can write down a likelihood function for our GP

$$
\log(p(y|X)) = -\frac{1}{2}\log y^T \left( \mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}y  -  \frac{1}{2}\log \mid \mathbf{K} + \sigma^2 \mathbf{I} \mid - \frac{n}{2}\log2\pi
$$

and we can optimise by this gradient descent with respect to $\sigma$ and $l$. So let's wrap things up by doing that for a toy problem. To start out we'll draw some data from a hidden function

```python
def f(x):
        return 2*np.sin(3*x) - np.cos(5*x) + x**2

def get_data(n_points):
        x = np.random.uniform(-2, 2, n_points)
        x.sort()
        u = 0.5
        y =  f(x) + np.random.randn(n_points) * u
        return x, y, u
```

Then code up that messy log likelihood there

```python
def logl(theta, x, y, u, x_test):
        x = x.reshape(-1, 1)
        K = sq_exp_kernel(x, x, theta) + np.eye(len(x))*u**2
        y = y.reshape(-1, 1)
        L =    ( - 0.5 * np.dot(np.dot(y.T, np.linalg.inv(K)), y)
                 - 0.5 * np.log(np.linalg.det(K + 1e-6 * np.eye(len(x))))
                 - 0.5 * len(x) * np.log(2 * np.pi))[0][0]

        return L if np.isfinite(L) else 1e25
```

In general this is a terrible function, and you definitely don't want to do things this way. Computing the determinent of $\mathbf{K}$ is a really bad call, the complexity goes as something bigger than $\mathcal{O}(n^{2.3})$, and the conditioning is terrible, even for this toy problem you'll notice that I'm adding a small constant to the diagonal to try to keep things from breaking. Still, it'll do for this example. Last thing is to stick this into an optimiser and solve for $\theta = (\sigma, l)$. Since most optimisers are minimisers, but we want the maximum likelihood, we'll take the negative of this quantity and optimse that

```python
nll = lambda *args: -1 * logl(*args)
result = opt.minimize(nll, [1, 1], args=(x, y, u, x_s))
```

Done. And presuming that didn't fall over we should get something like this

![gp_fit](/images/gp/gp_fit.png)

We can see that the draw from our GP match the data pretty well throughout, spreading in the region where we have no data to indicate our uncertainty in this region. All of this code can be found [here](https://github.com/neal-o-r/gp/blob/master/example_gp.py).

Now that we've rolled our own GP I feel clearer on what's going on inside of them. In the next (and last, I promise) post I'll cover using proper GP packages to fit models to real data.

---

[^1]: So long as our priors on the weights are Gaussian.

[^2]: An interesting thing is that when working with GPs you're mostly looking at Gaussians which have a mean of zero, all of the important stuff is happening in the covariance.

[^3]: That is to say these variables aren't independent.

[^4]: There are some conditions, positive-definiteness for instance.

[^5]: It's worth [reading](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf) a bit about the counter-intuitive properties of high-dimensional spaces to get an idea of why GPs behave the way they do.

[^6]: Alright, that was super hand-wavy. Suffice to say it's related to another nice property of Gaussians, the marginalisation property. More details [here](http://www.gaussianprocess.org/gpml/chapters/RW2.pdf).

[^7]: Coding this up this way is actually a really bad idea, you'll hit all sorts of bother with matrix conditioning in that inverse, but it'll do for this example.
