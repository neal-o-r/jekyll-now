---
layout: post
title: Introduction to Gaussian Processes, 3
---

In this post I want to wrap up the short series on GPs by discussing the common GP packages in Python, and showing how to model some real data.

There are a lot of GP packages available in Python, you can find a pretty thorough comparison of a couple of the big ones in this [paper](https://arxiv.org/pdf/1710.03157.pdf). Briefly, and far from exhaustively, here are a couple of the GP tools

* <tt>scikit-learn</tt>:  Scikit is the fist thing that comes to mind when you think of ML in Python, and conveniently it has a GP implementation. It doesn't provide very many kernels out of the box, but you can add your own pretty easily.

* <tt>GPy / GFlow</tt>: GPy was developed by the group at Sheffield, and GPFlow is a reimplementation of GPy with a TensorFlow backend. It's a pretty state-of-the-art tool box for all things GP, with a huge number of algorithms for applying GPs.

* <tt>PyMC3</tt>: PyMC3 is a Bayesian modelling package, built on top of Theano. It can do a whole bunch of very cool things, including GPs. Like GPFlow it provides functionality to fit GPs with MCMC, and as a result you can fit 'fully Bayesian' models and do inference on the hyperparameters.

In the examples below I'll use none of these however, I'll be using a package called [George](http://george.readthedocs.io/en/latest/) instead. George was developed for astrophysical applications and as a result it's the package I'm most familiar with. George is built using a particular algorithm (well written up in this [paper](https://arxiv.org/pdf/1403.6015.pdf)), that exploits the structure inherent to the kernel matrices to get some big speed-ups.[^1]

So as a quick example I'm going to fit a GP to some data using George. The dataset that I'll use is temperature measurements taken in my home town, Carlow. The measurements are taken every day but I'll take a the weekly mean, and use the standard deviation as the uncertainty. We have almost 15 years worth of measurements,

![weather](/images/gp/carlow_weather.png)

So our dataset is temperature, $y$, vs week number, $x$, with some uncertainty attached to each point, $u$. The most noticable feature of this data set is the periodicity, unsuprisingly it gets hot in the summer and cool in the winter. Since we're dealing with data with a very clear periodicity we want to pick a kernel that will reflect this. The periodic kernel in George is,

$$
k(x, x') = \cos\left(\frac{2\pi}{P}\mid x - x' \mid\right)
$$

This kernel will have a structure whereby points that are a distance of $1/P$ from one another will have a large covariance, like this

<center>
<img src='/images/gp/periodic.png'>
</center>

We can instantiate that kernel in George and evaluate the log-likelihood of our data under this model, as well as its gradient,

```python
kernel = 10 * kernels.CosineKernel(1/50)

gp = george.GP(kernel, mean=np.mean(y))
gp.compute(x, yerr=u)
print(gp.lnlikelihood(y))
print(gp.grad_lnlikelihood(y))
```

In reality it doesn't really make sense to use just the periodic kernel, you'd probably want to take the product of this kernel with something else to allow for quasi-periodic oscillations. Using this kernel alone is equivalent to fitting a simple periodic model to the data, but that's enough for the sake of this example.

Now that we've specified a model in George we can optimise the parameters of the kernel (the scaling factor and the period). As in our home-grown GP in the last post we'll do this using the minimiser in <tt>scipy</tt>

```python
def nll(p):
    gp.kernel.parameter_vector = p
    ll = gp.lnlikelihood(y, quiet=True)
    return -ll if np.isfinite(ll) else 1e25

def grad_nll(p):
    gp.kernel.parameter_vector = p
    return -gp.grad_lnlikelihood(y, quiet=True)

gp.compute(x, yerr=u)
p0 = gp.kernel.parameter_vector
results = opt.minimize(nll, p0, jac=grad_nll)
```
This shouldn't have too much trouble getting to a solution, provided you've made a judicious choice for the initial parameters. In the end we get something like this,

![fit](/images/gp/model_fit.png)

We also get an estimate of the periodicity of our data of 52.5 weeks, which seems fairly reasonable to me.

GPs is a huge area, and growing all the time as they see more and more use in fields like astrophysics. They're powerful, flexible, and are incredibly useful in situations where we want to accurately quantify uncertainty, or where we want to learn from small datasets. In these blog posts I've only covered the headlines, what they are, how they work, and why they're useful, and hopefully I've done it in a way that was easy enough to follow. If you want to know more about the topic you can't do a lot better than Rasmussen and Williams [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/) book, which is brilliant and, conveniently, is free online.  


---

[^1]: The same authors have provided another package called Celerite, which is very cool and ridiculously fast. Well worth a read of this [paper](https://arxiv.org/pdf/1703.09710.pdf)
